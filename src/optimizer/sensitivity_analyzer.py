# trading_system/src/optimizer/sensitivity_analyzer.py

"""
Sensitivity analysis module for trading strategy parameters based on portfolio performance.

Analyzes the impact of parameter perturbations on portfolio-level metrics
by leveraging the StrategyOptimizer's evaluation capabilities.
"""

import json
import logging
import traceback
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import mlflow
import numpy as np
import pandas as pd
from scipy.stats import spearmanr
from tqdm.auto import \
    tqdm  # Use auto version for better notebook compatibility

# Assuming MetricsDict is defined in performance_evaluator
# Assuming StrategyOptimizer is in the same directory or accessible via path
from src.optimizer.strategy_optimizer import StrategyOptimizer

# Configure module-level logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

class SensitivityAnalyzer:
    """
    Conducts sensitivity analysis on strategy parameters using portfolio metrics.

    Perturbs parameters around a baseline set and evaluates the impact on
    portfolio performance metrics obtained via the StrategyOptimizer's
    walk-forward cross-validation method.
    """

    def __init__(
        self,
        strategy_optimizer: StrategyOptimizer,
        base_params: Dict[str, Any],
        numeric_perturbation: float = 0.10,
        num_samples_per_param: int = 5, # Samples per parameter for 1-at-a-time analysis
        num_random_samples: int = 20, # Additional random combination samples
        parallel: bool = True
    ):
        """
        Initialize the sensitivity analyzer.

        Args:
            strategy_optimizer (StrategyOptimizer): Instance of StrategyOptimizer
                configured for the strategy and data. Will use its `walk_forward_cv`.
            base_params (Dict[str, Any]): Baseline (e.g., optimized) parameters.
            numeric_perturbation (float): Relative perturbation factor for numeric
                parameters (e.g., 0.1 means +/- 10%). Default is 0.10.
            num_samples_per_param (int): Number of samples generated by varying
                each numeric parameter individually (e.g., 5 means base +/- 1*perturb, base +/- 2*perturb). Default 5.
            num_random_samples (int): Number of additional samples where multiple
                parameters are randomly perturbed simultaneously. Default 20.
            parallel (bool): Use parallel processing for evaluating samples via
                the optimizer's cache/evaluation mechanism. Default True.
        """
        if not isinstance(strategy_optimizer, StrategyOptimizer):
             raise TypeError("strategy_optimizer must be an instance of StrategyOptimizer")
        if not base_params:
             raise ValueError("base_params cannot be empty.")
        if not 0 < numeric_perturbation < 1:
             logger.warning(f"numeric_perturbation ({numeric_perturbation}) is outside the typical (0, 1) range.")

        self.optimizer = strategy_optimizer
        self.base_params = base_params.copy()
        self.numeric_perturbation = numeric_perturbation
        self.num_samples_per_param = max(1, num_samples_per_param)
        self.num_random_samples = max(0, num_random_samples)
        self.parallel = parallel # Note: Parallelism now mainly happens within optimizer's walk_forward_cv if n_jobs > 1

        # Identify numeric parameters from the base_params
        self.numeric_params = {
            k: v for k, v in base_params.items()
            if isinstance(v, (int, float)) and not isinstance(v, bool) # Exclude booleans
        }
        # Store non-numeric params (passed through unchanged)
        self.non_numeric_params = {
            k: v for k, v in base_params.items()
            if k not in self.numeric_params
        }

        if not self.numeric_params:
             logger.warning("No numeric parameters found in base_params. Sensitivity analysis might be limited.")
        else:
             logger.info(f"Identified {len(self.numeric_params)} numeric parameters for sensitivity analysis: {list(self.numeric_params.keys())}")


    def _generate_perturbations(self) -> List[Dict[str, Any]]:
        """
        Generate perturbed parameter samples around the base parameters.

        Includes:
        1. The base parameters themselves.
        2. One-at-a-time perturbations for each numeric parameter across a range.
        3. Random combinations of perturbations across multiple parameters.

        Returns:
            List[Dict[str, Any]]: List of unique parameter sets to evaluate.
        """
        samples_set = set() # Use a set to store tuples of params to ensure uniqueness

        # Function to add a sample to the set after converting to tuple
        def add_sample(param_dict):
            # Ensure order for hashing/comparison
            param_tuple = tuple(sorted(param_dict.items()))
            samples_set.add(param_tuple)

        # 1. Add the base parameters
        add_sample(self.base_params)

        # 2. One-at-a-time perturbations
        if self.numeric_params:
            steps = np.linspace(-self.numeric_perturbation, self.numeric_perturbation, self.num_samples_per_param)
            for param_name, base_value in self.numeric_params.items():
                for step in steps:
                    if abs(step) < 1e-9: continue # Skip the base value itself (already added)

                    perturbed_value = base_value * (1 + step)

                    # Handle integer parameters - round and ensure minimum value (e.g., 1)
                    if isinstance(base_value, int):
                        perturbed_value = max(1, round(perturbed_value)) # Ensure positive integer, min 1
                        # Avoid adding if perturbation results in the same integer value
                        if perturbed_value == base_value: continue
                    elif perturbed_value <= 0 and base_value > 0:
                         perturbed_value = base_value * 0.01 # Don't let positive params go to zero/negative easily

                    sample = self.base_params.copy()
                    sample[param_name] = perturbed_value
                    add_sample(sample)

        # 3. Random combined perturbations
        if self.numeric_params and self.num_random_samples > 0:
             rng = np.random.default_rng(seed=42) # For reproducibility
             attempts = 0
             added_random = 0
             max_attempts = self.num_random_samples * 5 # Try harder to find unique samples

             while added_random < self.num_random_samples and attempts < max_attempts:
                 attempts += 1
                 sample = self.base_params.copy()
                 perturbed_count = 0
                 for param_name, base_value in self.numeric_params.items():
                     # Randomly decide whether to perturb this param (e.g., 50% chance)
                     if rng.random() < 0.5: continue

                     perturb_factor = rng.uniform(-self.numeric_perturbation, self.numeric_perturbation)
                     perturbed_value = base_value * (1 + perturb_factor)

                     if isinstance(base_value, int):
                         perturbed_value = max(1, round(perturbed_value))
                         if perturbed_value == base_value: continue # Skip if no change
                     elif perturbed_value <= 0 and base_value > 0:
                          perturbed_value = base_value * 0.01

                     sample[param_name] = perturbed_value
                     perturbed_count += 1

                 # Only add if at least one param was perturbed and it's a new combination
                 if perturbed_count > 0:
                      param_tuple = tuple(sorted(sample.items()))
                      if param_tuple not in samples_set:
                           samples_set.add(param_tuple)
                           added_random += 1

        # Convert set of tuples back to list of dicts
        final_samples = [dict(param_tuple) for param_tuple in samples_set]
        logger.info(f"Generated {len(final_samples)} unique parameter samples for sensitivity analysis.")
        return final_samples

    def _evaluate_sample(self, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Evaluate a single parameter sample using the optimizer's walk_forward_cv.

        Args:
            params (Dict[str, Any]): The parameter set to evaluate.

        Returns:
            Optional[Dict[str, Any]]: A dictionary containing the parameters and
                                      resulting portfolio metrics, or None if evaluation fails.
        """
        try:
            # Use the optimizer's evaluation method (which handles caching)
            portfolio_metrics, _ = self.optimizer.walk_forward_cv(params) # Ignore ticker metrics here

            if portfolio_metrics is None:
                logger.warning(f"Evaluation failed or returned no metrics for params: {params}")
                return None

            # Create result dictionary
            result = {}
            # Add parameters with 'param_' prefix
            for p_name, p_val in params.items():
                result[f'param_{p_name}'] = p_val
            # Add portfolio metrics with 'metric_' prefix
            for m_name, m_val in portfolio_metrics.items():
                result[f'metric_{m_name}'] = m_val

            return result

        except Exception as e:
            logger.error(f"Error evaluating sample with params {params}: {e}")
            logger.error(traceback.format_exc())
            return None

    def run(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Run the sensitivity analysis.

        Generates parameter samples, evaluates them using the optimizer's
        walk-forward CV (leveraging its parallelism and caching), calculates
        correlations, and returns reports.

        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]:
                - sensitivity_results_df: DataFrame with parameters and portfolio
                  metrics for each evaluated sample.
                - parameter_impact_df: DataFrame showing correlation (Spearman)
                  between each numeric parameter and key portfolio metrics.
        """
        analysis_run_name = f"SensitivityAnalysis_{self.optimizer.strategy_class.__name__}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        with mlflow.start_run(run_name=analysis_run_name, nested=True): # Nested under optimizer run if applicable
            logger.info(f"Starting sensitivity analysis run: {analysis_run_name}")
            mlflow.log_param("base_params", json.dumps(self.base_params, default=str))
            mlflow.log_param("numeric_perturbation", self.numeric_perturbation)
            mlflow.log_param("num_samples_per_param", self.num_samples_per_param)
            mlflow.log_param("num_random_samples", self.num_random_samples)

            # 1. Generate Samples
            samples = self._generate_perturbations()
            mlflow.log_param("total_samples_generated", len(samples))

            # 2. Evaluate Samples
            results = []
            logger.info(f"Evaluating {len(samples)} parameter samples...")

            # Evaluation leverages the optimizer's walk_forward_cv, which itself might be parallel.
            # Running _evaluate_sample in another layer of parallelism might be excessive or cause issues.
            # Let's run sequentially here, relying on the optimizer's internal parallelism.
            # If optimizer n_jobs=1, then consider uncommenting the ProcessPoolExecutor here.

            # --- Sequential Evaluation (Recommended if optimizer uses n_jobs > 1) ---
            for params in tqdm(samples, desc="Evaluating Sensitivity Samples"):
                 result = self._evaluate_sample(params)
                 if result:
                      results.append(result)

            # --- Optional: Parallel Evaluation Here (Use if optimizer n_jobs = 1) ---
            # if self.parallel and len(samples) > 1 and self.optimizer.n_jobs == 1:
            #     logger.info("Using ProcessPoolExecutor for sensitivity sample evaluation.")
            #     with ProcessPoolExecutor(max_workers=self.optimizer.n_jobs if self.optimizer.n_jobs > 0 else cpu_count()) as executor:
            #         future_to_params = {executor.submit(self._evaluate_sample, params): params for params in samples}
            #         for future in tqdm(as_completed(future_to_params), total=len(samples), desc="Evaluating Samples (Parallel)"):
            #             try:
            #                 res = future.result()
            #                 if res:
            #                     results.append(res)
            #             except Exception as exc:
            #                 params = future_to_params[future]
            #                 logger.error(f'Sample evaluation generated an exception for params {params}: {exc}')
            #                 logger.error(traceback.format_exc())
            # else: # Sequential if not parallel or optimizer is already parallel
            #     logger.info("Using sequential evaluation for sensitivity samples.")
            #     for params in tqdm(samples, desc="Evaluating Sensitivity Samples (Sequential)"):
            #         result = self._evaluate_sample(params)
            #         if result:
            #             results.append(result)
            # --- End Optional Parallel ---


            if not results:
                logger.error("No successful sample evaluations. Cannot perform sensitivity analysis.")
                return pd.DataFrame(), pd.DataFrame()

            # 3. Create Results DataFrame
            sensitivity_df = pd.DataFrame(results)
            logger.info(f"Successfully evaluated {len(sensitivity_df)} samples.")

            # Identify parameter and metric columns
            param_cols = [col for col in sensitivity_df.columns if col.startswith('param_') and col.replace('param_', '') in self.numeric_params]
            metric_cols = [col for col in sensitivity_df.columns if col.startswith('metric_')]

            if not param_cols:
                 logger.warning("No numeric parameter columns found in results. Cannot calculate impact.")
                 return sensitivity_df, pd.DataFrame()
            if not metric_cols:
                 logger.warning("No metric columns found in results. Cannot calculate impact.")
                 return sensitivity_df, pd.DataFrame()


            # 4. Calculate Parameter Impact (Correlation)
            impact_data = []
            target_metrics = [ # Select key metrics for impact analysis
                'metric_harmonic_mean', 'metric_sharpe_ratio', 'metric_annualized_return',
                'metric_max_drawdown', 'metric_annualized_volatility', 'metric_signal_accuracy'
            ]
            # Ensure target metrics actually exist in the results
            available_target_metrics = [m for m in target_metrics if m in sensitivity_df.columns]

            for p_col in param_cols:
                param_name = p_col.replace('param_', '')
                row = {'parameter': param_name}

                # Check for variance in the parameter column
                if sensitivity_df[p_col].nunique() < 2:
                     logger.debug(f"Parameter '{param_name}' has no variation in results. Skipping correlation.")
                     for m_col in available_target_metrics:
                          metric_name = m_col.replace('metric_', '')
                          row[f'{metric_name}_corr'] = np.nan
                          row[f'{metric_name}_pval'] = np.nan
                     impact_data.append(row)
                     continue

                for m_col in available_target_metrics:
                    metric_name = m_col.replace('metric_', '')
                    # Ensure metric column also has variance
                    if sensitivity_df[m_col].nunique() < 2:
                         logger.debug(f"Metric '{metric_name}' has no variation. Skipping correlation with '{param_name}'.")
                         row[f'{metric_name}_corr'] = np.nan
                         row[f'{metric_name}_pval'] = np.nan
                         continue

                    # Calculate Spearman correlation (robust to non-linear relationships)
                    try:
                         # Drop rows where either column is NaN for correlation calculation
                         valid_data = sensitivity_df[[p_col, m_col]].dropna()
                         if len(valid_data) < 3: # Need at least 3 points for meaningful correlation
                              corr, p_value = np.nan, np.nan
                         else:
                              corr, p_value = spearmanr(valid_data[p_col], valid_data[m_col])
                              if np.isnan(corr): # Handle cases where spearmanr returns nan (e.g., zero variance after dropna)
                                   corr, p_value = np.nan, np.nan

                         row[f'{metric_name}_corr'] = corr
                         row[f'{metric_name}_pval'] = p_value
                    except Exception as corr_err:
                        logger.warning(f"Could not calculate Spearman correlation for {param_name} and {metric_name}: {corr_err}")
                        row[f'{metric_name}_corr'] = np.nan
                        row[f'{metric_name}_pval'] = np.nan

                impact_data.append(row)

            parameter_impact_df = pd.DataFrame(impact_data).set_index('parameter')

            # 5. Save and Log Artifacts
            try:
                results_csv = f"sensitivity_analysis_{self.optimizer.strategy_class.__name__}_results.csv"
                impact_csv = f"sensitivity_analysis_{self.optimizer.strategy_class.__name__}_impact.csv"

                sensitivity_df.to_csv(results_csv, index=False)
                parameter_impact_df.to_csv(impact_csv)

                mlflow.log_artifact(results_csv)
                mlflow.log_artifact(impact_csv)
                logger.info(f"Saved sensitivity reports: {results_csv}, {impact_csv}")

            except Exception as io_error:
                logger.error(f"Error saving sensitivity analysis artifacts: {io_error}")

            return sensitivity_df, parameter_impact_df