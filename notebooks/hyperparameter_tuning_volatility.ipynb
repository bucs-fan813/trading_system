{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52fc8b0a",
   "metadata": {},
   "source": [
    "# ATR Trailing Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe00cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Know Sure Thing strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.volatility.atr_trailing_stops_strat import ATRTrailingStops\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204dbccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single stock\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = 10 # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 3\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = 1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for Awesome Oscillator ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import atr_trailing_stops_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting ATR Trailing Stops Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "        # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"ATR Trailing Stops_index_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {ATRTrailingStops.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class=ATRTrailingStops,\n",
    "            db_config=db_config,\n",
    "            search_space=atr_trailing_stops_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"ATR_Trailing Stops_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6833cae",
   "metadata": {},
   "source": [
    "# Bollinger Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Know Sure Thing strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.volatility.bollinger_bands_strat import BollingerBandsStrategy\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1011d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single stock\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = 10 # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 3\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = 1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for Awesome Oscillator ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import bollinger_bands_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting Bollinger Bands Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "        # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"Bollinger Bands_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {BollingerBandsStrategy.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class=BollingerBandsStrategy,\n",
    "            db_config=db_config,\n",
    "            search_space=bollinger_bands_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"Bollinger_bands_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f912e5a",
   "metadata": {},
   "source": [
    "# Garch Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Know Sure Thing strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.volatility.garch_strat import GARCHModel\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d574ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single stock\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = 10 # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 3\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = 1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for Awesome Oscillator ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import garch_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting Garch Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "        # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"Garch_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {GARCHModel.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class=GARCHModel,\n",
    "            db_config=db_config,\n",
    "            search_space=garch_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"Garch_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43271f5",
   "metadata": {},
   "source": [
    "# Keltner Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9869af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Know Sure Thing strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.volatility.keltner_channel_strat import KeltnerChannelStrategy\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single stock\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = 10 # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 3\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = 1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for Awesome Oscillator ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import keltner_channel_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting Keltner Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "        # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"Keltner_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {KeltnerChannelStrategy.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class=KeltnerChannelStrategy,\n",
    "            db_config=db_config,\n",
    "            search_space=keltner_channel_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"Keltner_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea446ffe",
   "metadata": {},
   "source": [
    "# SuperTrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4832a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Know Sure Thing strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.volatility.supertrend_strat import SupertrendStrategy\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a171b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single stock\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = 10 # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 3\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = 1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for Awesome Oscillator ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import supertrend_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting Supertrend Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "        # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"Supertrend_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {SupertrendStrategy.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class=SupertrendStrategy,\n",
    "            db_config=db_config,\n",
    "            search_space=supertrend_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"Supertrend_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
