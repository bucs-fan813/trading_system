{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aswesome Oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trading_system/scripts/run_ao_optimization.py\n",
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Awesome Oscillator strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.momentum.awesome_oscillator_strat import AwesomeOscillatorStrategy\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single stock\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = None # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 3\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = -1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for Awesome Oscillator ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import awesome_oscillator_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting Awesome Oscillator Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "        # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"AwesomeOscillator_Optimization_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {AwesomeOscillatorStrategy.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class=AwesomeOscillatorStrategy,\n",
    "            db_config=db_config,\n",
    "            search_space=awesome_oscillator_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"AwesomeOscillator_Opt_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coppock Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Coppock Oscillator strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.momentum.coppock_curve_strat import CoppockCurveStrategy\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = None # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 5\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = -1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for Coppock ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import coppock_curve_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting Coppock Oscillator Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"AwesomeOscillator_Optimization_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {CoppockCurveStrategy.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class=CoppockCurveStrategy,\n",
    "            db_config=db_config,\n",
    "            search_space=coppock_curve_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"CoppockOscillator_Opt_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Just the Strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = None # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "\n",
    "db_config = DatabaseConfig.default()\n",
    "\n",
    "cc = CoppockCurveStrategy(db_config, params = {'long_only': True, 'method': 'zero_crossing', 'normalize_strength': True, 'roc1_months': 10, 'roc2_months': 8, 'stop_loss_pct': 0.11588214818083328, 'strength_window': 378, 'sustain_days': 6, 'take_profit_pct': 0.06368548764576594, 'trailing_stop_pct': 0.04324839596707414, 'trend_strength_threshold': 0.7830082690475033, 'wma_lookback': 14, 'slippage_pct': 0.001, 'transaction_cost_pct': 0.001})\n",
    "\n",
    "a = cc.generate_signals(ticker=tickers_to_run, start_date=START_DATE, end_date=END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.signal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[a.signal == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.groupby('ticker').size().reset_index(name='counts').sort_values(by='counts', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Know Sure Thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Know Sure Thing strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.momentum.know_sure_thing_strat import KSTStrategy\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = None # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 5\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = -1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for KST ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import know_sure_thing_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting Know Sure Thing Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"AwesomeOscillator_Optimization_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {KSTStrategy.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class=KSTStrategy,\n",
    "            db_config=db_config,\n",
    "            search_space=know_sure_thing_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"kst_opt_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Know Sure Thing strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.momentum.macd_strat import MACDStrategy\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = 10 # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 5\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = -1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for KST ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import mcad_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting Know Sure Thing Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"MACD_Optimization_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {MACDStrategy.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class=MACDStrategy,\n",
    "            db_config=db_config,\n",
    "            search_space=mcad_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"macd_opt_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Raw code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = None # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "\n",
    "db_config = DatabaseConfig.default()\n",
    "\n",
    "cc = CoppockCurveStrategy(db_config, params = {'long_only': True, 'method': 'zero_crossing', 'normalize_strength': True, 'roc1_months': 10, 'roc2_months': 8, 'stop_loss_pct': 0.11588214818083328, 'strength_window': 378, 'sustain_days': 6, 'take_profit_pct': 0.06368548764576594, 'trailing_stop_pct': 0.04324839596707414, 'trend_strength_threshold': 0.7830082690475033, 'wma_lookback': 14, 'slippage_pct': 0.001, 'transaction_cost_pct': 0.001})\n",
    "\n",
    "a = cc.generate_signals(ticker=tickers_to_run, start_date=START_DATE, end_date=END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Vigor Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Know Sure Thing strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.momentum.relative_vigor_index_strat import RVIStrategy\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = None # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 5\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = -1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for MACD ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import relative_vigor_index_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting RVI Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"MACD_Optimization_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {RVIStrategy.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class= RVIStrategy,\n",
    "            db_config=db_config,\n",
    "            search_space=relative_vigor_index_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"kst_opt_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMA Crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Know Sure Thing strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.momentum.sma_crossover_strat import SMAStrategy\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = None # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 5\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = -1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for MACD ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import sma_crossover_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting RVI Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"SMA_Optimization_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {SMAStrategy.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class= SMAStrategy,\n",
    "            db_config=db_config,\n",
    "            search_space=sma_crossover_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"kst_opt_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSI Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main script to run hyperparameter optimization and sensitivity analysis\n",
    "for the Know Sure Thing strategy using the portfolio-based evaluation framework.\n",
    "\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "try:\n",
    "    from src.strategies.momentum.true_strength_index_strat import TSIStrategy\n",
    "    from src.optimizer.strategy_optimizer import StrategyOptimizer\n",
    "    from src.optimizer.sensitivity_analyzer import SensitivityAnalyzer\n",
    "    from src.database.config import DatabaseConfig\n",
    "    from utils.file_utils import load_tickers_from_yaml\n",
    "except ImportError as e:\n",
    "    print(\"Error importing modules. Make sure the script is run from the project root\")\n",
    "    print(\"or the 'src' directory is in the Python path.\")\n",
    "    print(f\"Import Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MLflow Configuration\n",
    "MLFLOW_TRACKING_URI = \"file:./mlruns\"  # Store MLflow data locally in ./mlruns\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Data Configuration\n",
    "TICKER_FILE_PATH = \"../data/tickers.yml\" # Path relative to project root\n",
    "MAX_TICKERS = None # Limit tickers for faster testing, set to None to use all\n",
    "\n",
    "# Backtest Period\n",
    "START_DATE = (datetime.now() - timedelta(days=4*365)).strftime(\"%Y-%m-%d\")\n",
    "END_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Optimization Settings\n",
    "CV_FOLDS = 5\n",
    "MAX_EVALS = 50  # Number of hyperparameter sets to evaluate\n",
    "OPTIMIZATION_METRIC = 'harmonic_mean' # Portfolio metric to maximize (minus penalty)\n",
    "N_JOBS = -1 # Use all available CPU cores for fold evaluation within optimizer\n",
    "\n",
    "# Sensitivity Analysis Settings\n",
    "RUN_SENSITIVITY = False # Set to False to skip sensitivity analysis\n",
    "NUMERIC_PERTURBATION = 0.15 # +/- 15% for sensitivity\n",
    "SENS_SAMPLES_PER_PARAM = 5\n",
    "SENS_RANDOM_SAMPLES = 20\n",
    "\n",
    "# --- Define Search Space for MACD ---\n",
    "\n",
    "# Note: Hyperopt doesn't easily enforce short_period < long_period directly during sampling.\n",
    "# The optimizer will evaluate invalid combinations, and they will likely fail or perform poorly.\n",
    "# Strategy itself raises ValueError if short >= long during initialization.\n",
    "\n",
    "from src.optimizer.search_space import true_strength_index_strat_search_space\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"--- Starting RVI Optimization Script ---\")\n",
    "\n",
    "    # Setup MLflow\n",
    "    try:\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "        logger.info(f\"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set MLflow tracking URI: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # Create MLflow experiment if it doesn't exist\n",
    "    try:\n",
    "        experiment_name = f\"TSI_Optimization_{RUN_TIMESTAMP}\"\n",
    "        # Check if experiment exists\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            # Create new experiment\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "            logger.info(f\"Created new MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        else:\n",
    "            experiment_id = experiment.experiment_id\n",
    "            logger.info(f\"Using existing MLflow experiment: {experiment_name} with ID: {experiment_id}\")\n",
    "        \n",
    "        # Set the experiment for subsequent runs\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create or set MLflow experiment: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load Tickers\n",
    "    try:\n",
    "        tickers_to_run = load_tickers_from_yaml(TICKER_FILE_PATH, MAX_TICKERS)\n",
    "    except Exception:\n",
    "        logger.error(\"Failed to load tickers. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Database Config\n",
    "    try:\n",
    "        db_config = DatabaseConfig.default()\n",
    "        # Optional: Add a check here to ensure DB connection is valid if possible\n",
    "        logger.info(\"Database configuration loaded.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load database configuration: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run Optimization ---\n",
    "    optimizer = None\n",
    "    best_params = {}\n",
    "    portfolio_performance_report = pd.DataFrame()\n",
    "    param_history_report = pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Initializing StrategyOptimizer for {TSIStrategy.__name__}\")\n",
    "    try:\n",
    "        optimizer = StrategyOptimizer(\n",
    "            strategy_class= TSIStrategy,\n",
    "            db_config=db_config,\n",
    "            search_space=true_strength_index_strat_search_space,\n",
    "            tickers=tickers_to_run,\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            cv_folds=CV_FOLDS,\n",
    "            max_evals=MAX_EVALS,\n",
    "            optimization_metric=OPTIMIZATION_METRIC,\n",
    "            run_name=f\"kst_opt_{RUN_TIMESTAMP}\",\n",
    "            n_jobs=N_JOBS\n",
    "            # risk_thresholds can be customized here if needed, otherwise defaults are used\n",
    "        )\n",
    "\n",
    "        logger.info(\"Starting hyperparameter optimization...\")\n",
    "        best_params, portfolio_performance_report, param_history_report = optimizer.run_optimization()\n",
    "\n",
    "        if not best_params:\n",
    "             logger.error(\"Optimization did not yield valid results. Best parameters not found.\")\n",
    "        else:\n",
    "             logger.info(\"--- Optimization Results ---\")\n",
    "             logger.info(f\"Best Parameters found:\\n{json.dumps(best_params, indent=2)}\")\n",
    "             logger.info(f\"\\nBest Portfolio Performance Report:\\n{portfolio_performance_report.to_string()}\")\n",
    "             logger.info(f\"\\nParameter History saved (see MLflow artifacts or CSV file). Head:\\n{param_history_report.head().to_string()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during optimization: {e}\", exc_info=True)\n",
    "        # Attempt to end MLflow run if it was started by the optimizer\n",
    "        if mlflow.active_run():\n",
    "            mlflow.end_run(\"FAILED\")\n",
    "\n",
    "    # --- Run Sensitivity Analysis (Optional) ---\n",
    "    if RUN_SENSITIVITY and optimizer and best_params:\n",
    "        logger.info(\"\\n--- Starting Sensitivity Analysis ---\")\n",
    "        try:\n",
    "            analyzer = SensitivityAnalyzer(\n",
    "                strategy_optimizer=optimizer, # Reuse optimizer for its config and evaluation cache\n",
    "                base_params=best_params,\n",
    "                numeric_perturbation=NUMERIC_PERTURBATION,\n",
    "                num_samples_per_param=SENS_SAMPLES_PER_PARAM,\n",
    "                num_random_samples=SENS_RANDOM_SAMPLES,\n",
    "                parallel=True # Relies on optimizer's internal parallelization/caching\n",
    "            )\n",
    "\n",
    "            sensitivity_results_df, parameter_impact_df = analyzer.run()\n",
    "\n",
    "            if sensitivity_results_df.empty:\n",
    "                 logger.warning(\"Sensitivity analysis did not produce results.\")\n",
    "            else:\n",
    "                logger.info(\"--- Sensitivity Analysis Results ---\")\n",
    "                logger.info(f\"Sensitivity Results saved (see MLflow artifacts or CSV file). Head:\\n{sensitivity_results_df.head().to_string()}\")\n",
    "                logger.info(f\"\\nParameter Impact Report (Correlation):\\n{parameter_impact_df.to_string()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during sensitivity analysis: {e}\", exc_info=True)\n",
    "            if mlflow.active_run():\n",
    "                 mlflow.end_run(\"FAILED\") # End sensitivity run if it crashed\n",
    "\n",
    "    elif RUN_SENSITIVITY and (not optimizer or not best_params):\n",
    "        logger.warning(\"Skipping sensitivity analysis because optimization failed or produced no best parameters.\")\n",
    "\n",
    "\n",
    "    # Ensure any lingering run is terminated cleanly\n",
    "    # Should not be necessary if 'with mlflow.start_run()' is used correctly inside modules\n",
    "    # try:\n",
    "    #     while mlflow.active_run():\n",
    "    #         logger.info(f\"Ending lingering MLflow run: {mlflow.active_run().info.run_id}\")\n",
    "    #         mlflow.end_run()\n",
    "    # except Exception:\n",
    "    #      pass # Ignore errors during cleanup\n",
    "\n",
    "    logger.info(\"--- Script Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
